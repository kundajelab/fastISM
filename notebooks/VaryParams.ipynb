{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vary Parameters\n",
    "\n",
    "For a given base architecture, vary different parameters and see the effect on speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import fastISM\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from importlib import reload\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 25 08:45:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.57.02    Driver Version: 470.57.02    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:03:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    28W / 250W |      0MiB / 16280MiB |      0%   E. Process |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-27db8534-9b2b-8b1a-5889-9c77c0c7be4e)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Wed_Apr_24_19:10:27_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.168\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GPU:0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'GPU:0' if tf.config.experimental.list_physical_devices('GPU') else '/device:CPU:0'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = np.load(\"test.seq.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extending\n",
    "seqs = np.hstack([seqs,seqs,seqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6000, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(seqlen=1000, numchars=4, num_outputs=1, num_channels=256, conv_width=10, num_convs=4):\n",
    "    inp = tf.keras.Input(shape=(seqlen, numchars))\n",
    "    x = inp\n",
    "    \n",
    "    for i in range(num_convs):        \n",
    "        x = tf.keras.layers.Conv1D(\n",
    "            num_channels, conv_width, strides=1, padding='same', activation='relu')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.MaxPool1D(2)(x)\n",
    "\n",
    "    # fc\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1000, activation='relu', name='fc1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(1000, activation='relu', name='fc2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(num_outputs, name='fc3')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inp, outputs=x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Best practice would be to restart kernel after benchmarking each model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_ism(ism_model, batch_sizes, seqlen):\n",
    "    times = []\n",
    "    per_100 = []\n",
    "    for b in batch_sizes:\n",
    "\n",
    "        # dry run -- required as first batch slower for setting up\n",
    "        #            and variable batch sizes (due to varying number\n",
    "        #            of seqs that need to be mutated at a position) \n",
    "        #            also slows down first call\n",
    "        # x = np.random.random((b,seqlen,4))\n",
    "        x = seqs[:b, :seqlen]        \n",
    "        x = tf.constant(x, dtype=ism_model.model.inputs[0].dtype)\n",
    "        o = ism_model(x, [0,0,0,1])\n",
    "        \n",
    "        t = time.time()\n",
    "        x = tf.constant(x, dtype=ism_model.model.inputs[0].dtype)\n",
    "        \n",
    "        # NOTE: computations are only performed at those positions\n",
    "        # at which the existing base != replace_with\n",
    "        o = ism_model(x, replace_with=[0,0,0,1])\n",
    "        o = ism_model(x, replace_with=[0,0,1,0])\n",
    "        o = ism_model(x, replace_with=[0,1,0,0])\n",
    "        o = ism_model(x, replace_with=[1,0,0,0])\n",
    "        \n",
    "        times.append(time.time()-t)\n",
    "        \n",
    "        per_100.append((times[-1]/b)*100)\n",
    "        print(\"BATCH: {}\\tTIME: {:.2f}\\tPER 100: {:.2f}\".format(b, times[-1], (times[-1]/b)*100))\n",
    "    \n",
    "    print(\"BEST PER 100: {:.2f}\".format(min(per_100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ LEN : 64\n",
      "BATCH: 4096\tTIME: 12.68\tPER 100: 0.31\n",
      "BEST PER 100: 0.31\n",
      "BATCH: 512\tTIME: 4.33\tPER 100: 0.85\n",
      "BEST PER 100: 0.85\n",
      "----\n",
      "SEQ LEN : 128\n",
      "BATCH: 4096\tTIME: 20.03\tPER 100: 0.49\n",
      "BEST PER 100: 0.49\n",
      "BATCH: 512\tTIME: 5.71\tPER 100: 1.12\n",
      "BEST PER 100: 1.12\n",
      "----\n",
      "SEQ LEN : 256\n",
      "BATCH: 4096\tTIME: 40.55\tPER 100: 0.99\n",
      "BEST PER 100: 0.99\n",
      "BATCH: 512\tTIME: 17.78\tPER 100: 3.47\n",
      "BEST PER 100: 3.47\n",
      "----\n",
      "SEQ LEN : 512\n",
      "BATCH: 4096\tTIME: 76.25\tPER 100: 1.86\n",
      "BEST PER 100: 1.86\n",
      "BATCH: 512\tTIME: 66.92\tPER 100: 13.07\n",
      "BEST PER 100: 13.07\n",
      "----\n",
      "SEQ LEN : 1024\n",
      "BATCH: 4096\tTIME: 182.41\tPER 100: 4.45\n",
      "BEST PER 100: 4.45\n",
      "BATCH: 512\tTIME: 245.77\tPER 100: 48.00\n",
      "BEST PER 100: 48.00\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for seqlen in [64,128,256,512,1024]:\n",
    "    print(\"SEQ LEN : {}\".format(seqlen))\n",
    "\n",
    "    model = base_model(seqlen=seqlen)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [4096], seqlen)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [512], seqlen)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ LEN : 2048\n",
      "BATCH: 2048\tTIME: 267.30\tPER 100: 13.05\n",
      "BEST PER 100: 13.05\n",
      "BATCH: 256\tTIME: 475.71\tPER 100: 185.82\n",
      "BEST PER 100: 185.82\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for seqlen in [2048]:\n",
    "    print(\"SEQ LEN : {}\".format(seqlen))\n",
    "\n",
    "    model = base_model(seqlen=seqlen)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [2048], 2048)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [256], 2048)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ LEN : 4096\n",
      "BATCH: 512\tTIME: 319.20\tPER 100: 62.34\n",
      "BEST PER 100: 62.34\n",
      "BATCH: 64\tTIME: 521.99\tPER 100: 815.61\n",
      "BEST PER 100: 815.61\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for seqlen in [4096]:\n",
    "    print(\"SEQ LEN : {}\".format(seqlen))\n",
    "\n",
    "    model = base_model(seqlen=seqlen)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [512], 4096)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [64], 4096)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Conv Width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANDIDATE_CONV_WIDTHS = [1,5,10,15,20,30,40,50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV WIDTH : 1\n",
      "BATCH: 4096\tTIME: 113.93\tPER 100: 2.78\n",
      "BEST PER 100: 2.78\n",
      "BATCH: 512\tTIME: 123.87\tPER 100: 24.19\n",
      "BEST PER 100: 24.19\n",
      "----\n",
      "CONV WIDTH : 5\n",
      "BATCH: 4096\tTIME: 156.98\tPER 100: 3.83\n",
      "BEST PER 100: 3.83\n",
      "BATCH: 512\tTIME: 191.88\tPER 100: 37.48\n",
      "BEST PER 100: 37.48\n",
      "----\n",
      "CONV WIDTH : 10\n",
      "BATCH: 4096\tTIME: 196.85\tPER 100: 4.81\n",
      "BEST PER 100: 4.81\n",
      "BATCH: 512\tTIME: 239.44\tPER 100: 46.77\n",
      "BEST PER 100: 46.77\n",
      "----\n",
      "CONV WIDTH : 15\n",
      "BATCH: 4096\tTIME: 298.51\tPER 100: 7.29\n",
      "BEST PER 100: 7.29\n",
      "BATCH: 512\tTIME: 234.13\tPER 100: 45.73\n",
      "BEST PER 100: 45.73\n",
      "----\n",
      "CONV WIDTH : 20\n",
      "BATCH: 4096\tTIME: 607.98\tPER 100: 14.84\n",
      "BEST PER 100: 14.84\n",
      "BATCH: 512\tTIME: 254.19\tPER 100: 49.65\n",
      "BEST PER 100: 49.65\n",
      "----\n",
      "CONV WIDTH : 30\n",
      "BATCH: 4096\tTIME: 689.36\tPER 100: 16.83\n",
      "BEST PER 100: 16.83\n",
      "BATCH: 512\tTIME: 259.03\tPER 100: 50.59\n",
      "BEST PER 100: 50.59\n",
      "----\n",
      "CONV WIDTH : 40\n",
      "BATCH: 4096\tTIME: 809.05\tPER 100: 19.75\n",
      "BEST PER 100: 19.75\n",
      "BATCH: 512\tTIME: 269.25\tPER 100: 52.59\n",
      "BEST PER 100: 52.59\n",
      "----\n",
      "CONV WIDTH : 50\n",
      "BATCH: 4096\tTIME: 962.85\tPER 100: 23.51\n",
      "BEST PER 100: 23.51\n",
      "BATCH: 512\tTIME: 269.39\tPER 100: 52.62\n",
      "BEST PER 100: 52.62\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for conv_width in CANDIDATE_CONV_WIDTHS:\n",
    "    print(\"CONV WIDTH : {}\".format(conv_width))\n",
    "\n",
    "    model = base_model(conv_width=conv_width)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [4096], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [512], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Number of Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CHANNELS : 32\n",
      "BATCH: 4096\tTIME: 62.42\tPER 100: 1.52\n",
      "BEST PER 100: 1.52\n",
      "BATCH: 512\tTIME: 30.33\tPER 100: 5.92\n",
      "BEST PER 100: 5.92\n",
      "----\n",
      "NUM CHANNELS : 64\n",
      "BATCH: 4096\tTIME: 75.61\tPER 100: 1.85\n",
      "BEST PER 100: 1.85\n",
      "BATCH: 512\tTIME: 54.83\tPER 100: 10.71\n",
      "BEST PER 100: 10.71\n",
      "----\n",
      "NUM CHANNELS : 128\n",
      "BATCH: 4096\tTIME: 116.60\tPER 100: 2.85\n",
      "BEST PER 100: 2.85\n",
      "BATCH: 512\tTIME: 107.96\tPER 100: 21.09\n",
      "BEST PER 100: 21.09\n",
      "----\n",
      "NUM CHANNELS : 256\n",
      "BATCH: 4096\tTIME: 194.74\tPER 100: 4.75\n",
      "BEST PER 100: 4.75\n",
      "BATCH: 512\tTIME: 237.54\tPER 100: 46.39\n",
      "BEST PER 100: 46.39\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for num_channels in [32,64,128,256]:\n",
    "    print(\"NUM CHANNELS : {}\".format(num_channels))\n",
    "\n",
    "    model = base_model(num_channels=num_channels)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [4096], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [512], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CHANNELS : 512\n",
      "BATCH: 2048\tTIME: 217.55\tPER 100: 10.62\n",
      "BEST PER 100: 10.62\n",
      "BATCH: 256\tTIME: 296.11\tPER 100: 115.67\n",
      "BEST PER 100: 115.67\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for num_channels in [512]:\n",
    "    print(\"NUM CHANNELS : {}\".format(num_channels))\n",
    "\n",
    "    model = base_model(num_channels=num_channels)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [2048], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [256], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CHANNELS : 1024\n",
      "BATCH: 1024\tTIME: 290.17\tPER 100: 28.34\n",
      "BEST PER 100: 28.34\n",
      "BATCH: 128\tTIME: 423.25\tPER 100: 330.67\n",
      "BEST PER 100: 330.67\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for num_channels in [1024]:\n",
    "    print(\"NUM CHANNELS : {}\".format(num_channels))\n",
    "\n",
    "    model = base_model(num_channels=num_channels)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [1024], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [128], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Varying Number of Conv Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CONVS : 1\n",
      "BATCH: 4096\tTIME: 542.97\tPER 100: 13.26\n",
      "BEST PER 100: 13.26\n",
      "BATCH: 512\tTIME: 129.08\tPER 100: 25.21\n",
      "BEST PER 100: 25.21\n",
      "----\n",
      "NUM CONVS : 2\n",
      "BATCH: 4096\tTIME: 314.00\tPER 100: 7.67\n",
      "BEST PER 100: 7.67\n",
      "BATCH: 512\tTIME: 185.16\tPER 100: 36.16\n",
      "BEST PER 100: 36.16\n",
      "----\n",
      "NUM CONVS : 3\n",
      "BATCH: 4096\tTIME: 214.54\tPER 100: 5.24\n",
      "BEST PER 100: 5.24\n",
      "BATCH: 512\tTIME: 218.98\tPER 100: 42.77\n",
      "BEST PER 100: 42.77\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for num_convs in [1,2,3]:\n",
    "    print(\"NUM CONVS : {}\".format(num_convs))\n",
    "\n",
    "    model = base_model(num_convs=num_convs)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [4096], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [512], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CONVS : 4\n",
      "BATCH: 4096\tTIME: 194.86\tPER 100: 4.76\n",
      "BEST PER 100: 4.76\n",
      "BATCH: 512\tTIME: 237.65\tPER 100: 46.42\n",
      "BEST PER 100: 46.42\n",
      "----\n",
      "NUM CONVS : 5\n",
      "BATCH: 4096\tTIME: 172.98\tPER 100: 4.22\n",
      "BEST PER 100: 4.22\n",
      "BATCH: 512\tTIME: 240.04\tPER 100: 46.88\n",
      "BEST PER 100: 46.88\n",
      "----\n",
      "NUM CONVS : 6\n",
      "BATCH: 4096\tTIME: 188.00\tPER 100: 4.59\n",
      "BEST PER 100: 4.59\n",
      "BATCH: 512\tTIME: 244.23\tPER 100: 47.70\n",
      "BEST PER 100: 47.70\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for num_convs in [4,5,6]:\n",
    "    print(\"NUM CONVS : {}\".format(num_convs))\n",
    "\n",
    "    model = base_model(num_convs=num_convs)\n",
    "\n",
    "    model_fism = fastISM.FastISM(model, test_correctness=False)\n",
    "    time_ism(model_fism, [4096], 1000)\n",
    "    \n",
    "    model_nism = fastISM.NaiveISM(model)\n",
    "    time_ism(model_nism, [512], 1000)\n",
    "    \n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
